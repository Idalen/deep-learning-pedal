{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fe1e842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([Float32[0.14421366 -0.11200254 … -0.12652631 0.27534384; 0.087245904 -0.19417483 … -0.1467813 0.017415175; … ; 0.11723644 -0.20972107 … 0.17602403 -0.26483983; -0.1698205 -0.27099916 … 0.06739609 -0.14771992], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[-0.14387336 -0.030527383 … -0.0121586025 -0.20781264; 0.1396108 0.016838282 … -0.21529499 -0.16745552; … ; -0.045312345 0.07205531 … 0.096075624 0.22652751; 0.01971072 0.11408073 … -0.15361717 -0.087421596], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.3892065 0.054219663 … 0.121739 0.3535251], Float32[0.0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "\n",
    "# Define the neural network class\n",
    "struct MyNeuralNetwork\n",
    "    layers::Vector{Any}\n",
    "end\n",
    "\n",
    "# Constructor for the neural network class\n",
    "function MyNeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "    layers = []\n",
    "    \n",
    "    push!(layers, Dense(input_size, hidden_size1, relu))\n",
    "    push!(layers, Dense(hidden_size1, hidden_size2, relu))\n",
    "    push!(layers, Dense(hidden_size2, output_size))\n",
    "                \n",
    "    return MyNeuralNetwork(layers)\n",
    "end\n",
    "\n",
    "# Forward pass method for the neural network class\n",
    "function (model::MyNeuralNetwork)(input)\n",
    "    output = input\n",
    "    \n",
    "    for (i, layer) in enumerate(model.layers)\n",
    "       output = layer(output) \n",
    "    end\n",
    "    \n",
    "    return output\n",
    "end\n",
    "\n",
    "Flux.@functor MyNeuralNetwork\n",
    "# Create an instance of the neural network\n",
    "input_size = 10\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 1\n",
    "\n",
    "loss(ŷ, y) = Flux.Losses.mse(ŷ, y)\n",
    "\n",
    "model = MyNeuralNetwork(input_size, hidden_size1, hidden_size2, output_size)\n",
    "opt = Flux.setup(Adam(0.5), model)\n",
    "\n",
    "Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4b95e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 adjoint(::Matrix{Int64}) with eltype Int64:\n",
       " 1  1  0  1  0  0  1  0  0  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rand(Float32, (10, 10))\n",
    "y = [1, 1, 0, 1, 0, 0, 1, 0, 0, 1]\n",
    "y=y[:, 1:1]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7a0ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.59699f0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(model(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166456ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((layers = NamedTuple{(:weight, :bias, :σ), Tuple{Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Matrix{Float32}, Matrix{Float32}, Tuple{Float64, Float64}}}, Optimisers.Leaf{Optimisers.Adam{Float64}, Tuple{Vector{Float32}, Vector{Float32}, Tuple{Float64, Float64}}}, Tuple{}}}[(weight = \u001b[32mLeaf(Adam{Float64}(0.5, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.000193751 0.00115305 … 9.54052f-5 -2.64044f-5; 0.00267921 0.00240734 … 0.00105155 0.00162945; … ; 114.989 91.6122 … 87.48 79.7926; 195.746 155.995 … 138.249 130.87], Float32[4.62987f-9 1.63975f-7 … 1.1226f-9 8.59868f-11; 8.85305f-7 7.14755f-7 … 1.36376f-7 3.27464f-7; … ; 1322.25 839.282 … 765.284 636.69; 3831.75 2433.5 … 1911.3 1712.72], (0.729, 0.997003))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.5, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.00167788, 0.00413826, 0.0, 321.893, 0.0, 0.0, 284.493, 0.0, 0.0, 0.000155346  …  0.000977307, 0.00158036, 321.342, 0.000118472, 0.0, 0.0, 0.00361689, 235.95, 173.383, 312.332], Float32[3.47218f-7, 2.1121f-6, 0.0, 10361.7, 0.0, 0.0, 8093.75, 0.0, 0.0, 2.97634f-9  …  1.17799f-7, 3.08031f-7, 10326.4, 1.73105f-9, 0.0, 0.0, 1.61343f-6, 5567.24, 3006.18, 9755.31], (0.729, 0.997003))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam{Float64}(0.5, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.000357859 -4.45898f-5 … 0.000136466 0.00113623; … ; -0.000335085 2.12235f-5 … 33.9512 116.214; -0.000674716 4.27349f-5 … 41.3213 141.441], Float32[0.0 0.0 … 0.0 0.0; 1.57945f-8 2.45217f-10 … 2.29683f-9 1.59226f-7; … ; 1.38481f-8 5.55541f-11 … 115.269 1350.59; 5.61465f-8 2.25241f-10 … 170.747 2000.61], (0.729, 0.997003))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.5, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0, 0.00445784, 0.00298183, 0.0, 0.00769163, 0.00110669, 0.00475991, 51.4433, 45.6477, 30.1441  …  0.0, -0.000225636, -28.0651, 0.000134155, 0.0119207, 0.00983482, 0.0, 34.868, 37.9808, 46.2219], Float32[0.0, 2.45092f-6, 1.09659f-6, 0.0, 7.29654f-6, 1.51053f-7, 2.79433f-6, 264.772, 208.433, 90.8681  …  0.0, 6.27907f-9, 78.7623, 2.21969f-9, 1.75261f-5, 1.19293f-5, 0.0, 121.579, 144.29, 213.736], (0.729, 0.997003))\u001b[32m)\u001b[39m, σ = ()), (weight = \u001b[32mLeaf(Adam{Float64}(0.5, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[0.0 0.000505606 … 1931.41 1921.16], Float32[0.0 3.15286f-8 … 3.73034f5 3.69085f5], (0.729, 0.997003))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.5, (0.9, 0.999), 1.0e-8), \u001b[39m(Float32[59.6646], Float32[356.407], (0.729, 0.997003))\u001b[32m)\u001b[39m, σ = ())],), MyNeuralNetwork(Dense{F, Matrix{Float32}, Vector{Float32}} where F[Dense(10 => 64, relu), Dense(64 => 32, relu), Dense(32 => 1)]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇ = Flux.gradient(model) do m\n",
    "                ŷ = m(x)\n",
    "                loss(ŷ, y)\n",
    "            end\n",
    "\n",
    "Flux.update!(opt, model, ∇[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd01da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1×1 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       " 0.21369666\n",
       " 0.27345043\n",
       " 0.86771923\n",
       " 0.976491\n",
       " 0.83404946\n",
       " 0.64452153\n",
       " 0.6240405\n",
       " 0.6735903\n",
       " 0.2613253\n",
       " 0.64536947"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rand(Float32, (10, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb0cda33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×1×1 Array{Float32, 3}:\n",
       "[:, :, 1] =\n",
       "  0.21369666\n",
       "  0.07043861\n",
       "  0.6079413\n",
       "  0.15215772\n",
       " -0.09361696\n",
       " -0.14782542\n",
       "  0.011745036\n",
       "  0.08075184\n",
       " -0.37858546\n",
       "  0.39711043"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff=0.95f0\n",
    "vcat(x[1,:,:], x[2:end, :, :] - coeff .* x[1:end-1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f45a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
